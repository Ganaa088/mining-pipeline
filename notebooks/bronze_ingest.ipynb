{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6de73466-ec2f-421c-8447-739bda31d464",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ACCOUNT = \"miningganaa01\"   \n",
    "SAS = \"sp=racwl&st=2025-08-29T09:42:27Z&se=2025-09-20T17:57:27Z&spr=https&sv=2024-11-04&sr=c&sig=1wxn%2BAjzhAFX0i7cIGiJPmwCMy2zW5OY9o%2Fh8ES6MMk%3D\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b424b25c-58f6-4627-8613-dbc9b21b74a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CONTAINER = \"dls\"\n",
    "BRONZE_DIR = f\"abfss://{CONTAINER}@{ACCOUNT}.dfs.core.windows.net/bronze/kaggle/quality_prediction\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4003e606-744d-42de-beda-e2792905f567",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(f\"fs.azure.sas.{CONTAINER}.{ACCOUNT}.dfs.core.windows.net\", SAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "417ac722-761f-4794-a608-c1e5d2557f80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ACCOUNT   = \"miningganaa01\"                     # your storage account\n",
    "CONTAINER = \"dls\"                               # your container\n",
    "SAS       = \"?sp=racwl&st=2025-08-29T09:42:27Z&se=2025-09-20T17:57:27Z&spr=https&sv=2024-11-04&sr=c&sig=1wxn%2BAjzhAFX0i7cIGiJPmwCMy2zW5OY9o%2Fh8ES6MMk%3D\" # paste EXACTLY (KEEP the leading \"?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7860cc28-5ff3-4243-a0a9-e078ac3a0da0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Tell the ABFS driver to use SAS for this account ---\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{ACCOUNT}.dfs.core.windows.net\", \"SAS\")\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.sas.token.provider.type.{ACCOUNT}.dfs.core.windows.net\",\n",
    "    \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\"\n",
    ")\n",
    "spark.conf.set(f\"fs.azure.sas.fixed.token.{ACCOUNT}.dfs.core.windows.net\", SAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a240ba5f-2d78-44fd-8aaf-3c0b17d5376a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Bronze path in ADLS (no SAS in the path when using FixedSASTokenProvider) ---\n",
    "BRONZE_DIR = f\"abfss://{CONTAINER}@{ACCOUNT}.dfs.core.windows.net/bronze/kaggle/quality_prediction\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "916aae00-12ba-4c2a-acdc-f16c83c8244b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Read all CSVs from Bronze ---\n",
    "df_raw = (spark.read\n",
    "          .option(\"header\", True)        # use first row as headers\n",
    "          .option(\"inferSchema\", True)   # auto-detect data types\n",
    "          .csv(f\"{BRONZE_DIR}/*\"))\n",
    "\n",
    "display(df_raw.limit(10))    # quick preview\n",
    "df_raw.printSchema()         # inspect column names + types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73cf2ca4-4be6-41a1-aa6f-34abc9f8cea8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Save a Bronze Delta copy for stability and time-travel ---\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS mining\")\n",
    "(df_raw.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(\"mining.bronze_flotation\"))\n",
    "\n",
    "print(\"Row count:\", spark.table(\"mining.bronze_flotation\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9aa114e-c99a-4788-a941-55883e6a0807",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# FIX: sanitize column names, then save BRONZE\n",
    "# ============================================\n",
    "from pyspark.sql import functions as F\n",
    "import re\n",
    "\n",
    "def normalize_col(colname: str) -> str:\n",
    "    \"\"\"\n",
    "    Make column names Delta/SQL friendly:\n",
    "    - lowercase\n",
    "    - replace '%' with 'percent'\n",
    "    - replace spaces/slashes with underscores\n",
    "    - collapse multiple underscores\n",
    "    - strip leading/trailing underscores\n",
    "    \"\"\"\n",
    "    s = colname.strip().lower()\n",
    "    s = s.replace(\"%\", \"percent\")\n",
    "    s = re.sub(r\"[\\/]\", \"_\", s)          # slashes -> underscore\n",
    "    s = re.sub(r\"\\s+\", \"_\", s)           # spaces -> underscore\n",
    "    s = re.sub(r\"__+\", \"_\", s)           # collapse repeats\n",
    "    s = s.strip(\"_\")\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a87cff04-5141-47c7-b454-f6282ac712ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Apply renames to a copy of df_raw\n",
    "df_bronze = df_raw\n",
    "rename_map = {c: normalize_col(c) for c in df_raw.columns}\n",
    "\n",
    "for old, new in rename_map.items():\n",
    "    if old != new:\n",
    "        df_bronze = df_bronze.withColumnRenamed(old, new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dcdc094-fc73-4b38-bf78-3b87b76f8b87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# standardize 'date' -> 'ts' (keeps it timestamp)\n",
    "if \"date\" in df_bronze.columns:\n",
    "    df_bronze = df_bronze.withColumnRenamed(\"date\", \"ts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "658f28fc-0904-4e35-bb06-3fa443cdb925",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save as Bronze Delta (now with safe column names)\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS mining\")\n",
    "(df_bronze.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(\"mining.bronze_flotation\"))\n",
    "\n",
    "print(\"Bronze saved. Row count:\", spark.table(\"mining.bronze_flotation\").count())\n",
    "spark.table(\"mining.bronze_flotation\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51dc2194-ca62-40e0-8115-eafe871f95a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ===========================================================\n",
    "# NEXT STEP: Create the SILVER layer (clean & standardized data)\n",
    "# ===========================================================\n",
    "# Business context:\n",
    "# The raw mining plant data (Bronze) is messy: all numbers are stored as text,\n",
    "# sometimes with commas or stray symbols. Before engineers or analysts can\n",
    "# trust the data, we need to clean and type-cast it.\n",
    "#\n",
    "# Why it matters:\n",
    "# Clean, numeric data enables quality checks, dashboards, and advanced models\n",
    "# (for example: tracking silica impurity trends that impact ore quality).\n",
    "# ===========================================================\n",
    "\n",
    "from pyspark.sql import functions as F, types as T\n",
    "import re\n",
    "\n",
    "# 1) Load Bronze\n",
    "bronze = spark.table(\"mining.bronze_flotation\")\n",
    "\n",
    "# 2) Ensure timestamp column is named ts \n",
    "df = bronze\n",
    "if \"ts\" not in df.columns and \"date\" in df.columns:\n",
    "    df = df.withColumnRenamed(\"date\", \"ts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57515586-b064-4faf-99fd-848f8f6c7633",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3) Identify which columns should be numeric (everything except 'ts' timestamp).\n",
    "numeric_cols = [c for c in df.columns if c != \"ts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83399df0-0f30-4b4f-878b-20bb2baec4d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4) Clean each numeric column:\n",
    "for c in numeric_cols:\n",
    "    # a) Remove leading/trailing spaces\n",
    "    df = df.withColumn(c, F.when(F.col(c).isNull(), None).otherwise(F.trim(F.col(c))))\n",
    "    # b) Convert European-style decimals (e.g. \"12,34\") into standard \"12.34\"\n",
    "    df = df.withColumn(c, F.regexp_replace(F.col(c), \",\", \".\"))\n",
    "    # c) Strip out any stray non-numeric symbols (e.g. '%')\n",
    "    df = df.withColumn(c, F.regexp_replace(F.col(c), r\"[^0-9\\.\\-]\", \"\"))\n",
    "    # d) Cast the cleaned string into a true DOUBLE data type\n",
    "    df = df.withColumn(c, F.col(c).cast(T.DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2abbf1c7-1a9b-491b-9a4a-7d4b3f00cace",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 5) Quick validation: how many rows, and how many nulls per column?\n",
    "row_count = df.count()\n",
    "nulls = (df.select([\n",
    "    F.sum(F.when(F.col(c).isNull(), 1).otherwise(0)).alias(c) for c in numeric_cols\n",
    "]).collect()[0].asDict())\n",
    "\n",
    "print(f\"[SILVER PREVIEW] Total rows = {row_count}\")\n",
    "print(\"[Sample of NULL counts by column]\")\n",
    "for k,v in list(nulls.items())[:5]:\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "461da415-07dd-4b72-9d98-02d46366f5ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 6) Save the clean result as the Silver Delta table.\n",
    "# This Silver layer is now ready for downstream analytics and KPI generation.\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS mining\")\n",
    "(df.write\n",
    "   .format(\"delta\")\n",
    "   .mode(\"overwrite\")              # overwrite for reproducibility\n",
    "   .option(\"overwriteSchema\", \"true\")\n",
    "   .saveAsTable(\"mining.silver_flotation\"))\n",
    "\n",
    "print(\"✅ Silver table saved as mining.silver_flotation\")\n",
    "display(df.limit(10))  # show a preview of the clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a43b0a1-25ef-4bfe-82cb-fa6944fd7994",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ===========================================================\n",
    "# STEP 9: Create the GOLD layer (business KPIs)\n",
    "# ===========================================================\n",
    "# Business context:\n",
    "# Engineers don’t want to stare at millions of rows of sensor data.\n",
    "# They need summarized metrics that answer key questions:\n",
    "#   - What was the average silica concentration each day?\n",
    "#   - Did silica ever exceed thresholds that hurt ore quality?\n",
    "#   - How stable was the process (min/max levels, records count)?\n",
    "#\n",
    "# Why it matters:\n",
    "# These Gold metrics become the backbone for dashboards and\n",
    "# decision-making — linking raw data to operational outcomes.\n",
    "# ===========================================================\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 1) Load the Silver table (already cleaned & typed)\n",
    "silver = spark.table(\"mining.silver_flotation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cf6f15f-fb21-403a-8d02-3d041cffa324",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2) Daily KPI aggregation\n",
    "daily_kpis = (silver\n",
    "              .withColumn(\"day\", F.to_date(\"ts\"))\n",
    "              .groupBy(\"day\")\n",
    "              .agg(\n",
    "                  F.avg(\"percent_silica_concentrate\").alias(\"avg_silica\"),\n",
    "                  F.max(\"percent_silica_concentrate\").alias(\"max_silica\"),\n",
    "                  F.min(\"percent_silica_concentrate\").alias(\"min_silica\"),\n",
    "                  F.count(\"*\").alias(\"records\")\n",
    "              )\n",
    "              .orderBy(\"day\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfcf40aa-191a-4e9f-8872-fef11547224b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3) Save Gold layer as a Delta table\n",
    "(daily_kpis.write\n",
    "   .format(\"delta\")\n",
    "   .mode(\"overwrite\")\n",
    "   .option(\"overwriteSchema\", \"true\")\n",
    "   .saveAsTable(\"mining.gold_daily_kpis\"))\n",
    "\n",
    "print(\"✅ Gold KPIs saved as mining.gold_daily_kpis\")\n",
    "display(daily_kpis.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8850b1cf-60b0-46e4-85c4-a2697dab5b38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ===========================================================\n",
    "# STEP 10: Add a threshold-based KPI to Gold\n",
    "# ===========================================================\n",
    "# Business context:\n",
    "# High silica content reduces iron ore quality and increases costs.\n",
    "# Engineers need to know not just averages, but how often\n",
    "# the process runs above critical thresholds (e.g., 2% silica).\n",
    "#\n",
    "# Why it matters:\n",
    "# This metric acts like a \"process stability score.\"\n",
    "# It highlights days where intervention may be needed.\n",
    "# ===========================================================\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Reload Silver (clean data)\n",
    "silver = spark.table(\"mining.silver_flotation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef5f9a3b-b001-458a-9e22-41fc9f7e5899",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Threshold value (domain knowledge: adjust if needed)\n",
    "THRESHOLD = 2.0\n",
    "\n",
    "# Compute daily KPI with % above threshold\n",
    "daily_kpis_threshold = (silver\n",
    "    .withColumn(\"day\", F.to_date(\"ts\"))\n",
    "    .groupBy(\"day\")\n",
    "    .agg(\n",
    "        F.avg(\"percent_silica_concentrate\").alias(\"avg_silica\"),\n",
    "        F.max(\"percent_silica_concentrate\").alias(\"max_silica\"),\n",
    "        F.min(\"percent_silica_concentrate\").alias(\"min_silica\"),\n",
    "        (F.sum(F.when(F.col(\"percent_silica_concentrate\") > THRESHOLD, 1).otherwise(0))\n",
    "         / F.count(\"*\") * 100).alias(\"pct_above_threshold\"),\n",
    "        F.count(\"*\").alias(\"records\")\n",
    "    )\n",
    "    .orderBy(\"day\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "395c8b8e-992e-4d97-aa55-07b0788f3423",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save updated Gold table\n",
    "(daily_kpis_threshold.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(\"mining.gold_daily_kpis\"))\n",
    "\n",
    "print(\"✅ Gold table updated with silica threshold KPI\")\n",
    "display(daily_kpis_threshold.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c217df6-d776-4baa-b9e2-57652da7036b",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1756464719730}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ===========================================================\n",
    "# RE-INGEST RAW CSV WITH EXPLICIT QUOTING OPTIONS\n",
    "# ===========================================================\n",
    "# Why: The file uses comma separators, AND commas inside numbers,\n",
    "# but fields are quoted. We tell Spark to respect quotes/escapes.\n",
    "\n",
    "ACCOUNT   = \"miningganaa01\"    # your account\n",
    "CONTAINER = \"dls\"\n",
    "BRONZE_DIR = f\"abfss://{CONTAINER}@{ACCOUNT}.dfs.core.windows.net/bronze/kaggle/quality_prediction\"\n",
    "\n",
    "df_raw = (spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", True)                 # first row has column names\n",
    "    .option(\"sep\", \",\")                     # comma-separated\n",
    "    .option(\"quote\", '\"')                   # fields are quoted with \"\n",
    "    .option(\"escape\", '\"')                  # quotes are escaped by doubling (\"\"), so escape is also \"\n",
    "    .option(\"multiLine\", False)             # lines are single-line\n",
    "    .option(\"ignoreLeadingWhiteSpace\", True)\n",
    "    .option(\"ignoreTrailingWhiteSpace\", True)\n",
    "    .option(\"inferSchema\", False)           # read as strings first; we’ll cast ourselves\n",
    "    .load(f\"{BRONZE_DIR}/*\"))\n",
    "\n",
    "display(df_raw.limit(3))\n",
    "df_raw.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21f89c2c-8e6f-452e-aeea-0da3bb977ec3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ===========================================================\n",
    "# BRONZE: safe column names, keep raw values (date stays string here)\n",
    "# ===========================================================\n",
    "import re\n",
    "def normalize_col(colname: str) -> str:\n",
    "    s = colname.strip().lower()\n",
    "    s = s.replace(\"%\", \"percent\")\n",
    "    s = re.sub(r\"[\\/]\", \"_\", s)\n",
    "    s = re.sub(r\"\\s+\", \"_\", s)\n",
    "    s = re.sub(r\"__+\", \"_\", s)\n",
    "    return s.strip(\"_\")\n",
    "\n",
    "df_bronze = df_raw\n",
    "for old, new in {c: normalize_col(c) for c in df_raw.columns}.items():\n",
    "    if old != new:\n",
    "        df_bronze = df_bronze.withColumnRenamed(old, new)\n",
    "\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS mining\")\n",
    "(df_bronze.write\n",
    "   .format(\"delta\")\n",
    "   .mode(\"overwrite\")\n",
    "   .option(\"overwriteSchema\",\"true\")\n",
    "   .saveAsTable(\"mining.bronze_flotation\"))\n",
    "\n",
    "print(\"✅ Bronze rebuilt\")\n",
    "spark.table(\"mining.bronze_flotation\").printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1015a73f-b098-4b6f-86cb-72868dbfdd7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ===========================================================\n",
    "# SILVER: parse timestamp + cast numerics\n",
    "# ===========================================================\n",
    "from pyspark.sql import functions as F, types as T\n",
    "\n",
    "bronze = spark.table(\"mining.bronze_flotation\")\n",
    "\n",
    "# a) Parse 'date' string -> proper timestamp 'ts'\n",
    "df = bronze.withColumn(\"ts\", F.to_timestamp(F.col(\"date\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "# b) Decide which columns should be numeric (all except 'date' & 'ts')\n",
    "numeric_cols = [c for c in df.columns if c not in (\"date\",\"ts\")]\n",
    "\n",
    "# c) Clean & cast numbers: handle comma decimals and stray symbols\n",
    "for c in numeric_cols:\n",
    "    df = df.withColumn(c, F.when(F.col(c).isNull(), None).otherwise(F.trim(F.col(c))))\n",
    "    df = df.withColumn(c, F.regexp_replace(F.col(c), \",\", \".\"))            # \"55,2\" -> \"55.2\"\n",
    "    df = df.withColumn(c, F.regexp_replace(F.col(c), r\"[^0-9\\.\\-]\", \"\"))   # strip %, spaces, etc.\n",
    "    df = df.withColumn(c, F.col(c).cast(T.DoubleType()))\n",
    "\n",
    "# d) Quick sanity check: ts should NOT be null now\n",
    "print(\"Null ts count:\", df.filter(F.col(\"ts\").isNull()).count())\n",
    "\n",
    "# e) Save Silver\n",
    "(df.select([\"ts\"] + numeric_cols)\n",
    "   .write\n",
    "   .format(\"delta\")\n",
    "   .mode(\"overwrite\")\n",
    "   .option(\"overwriteSchema\",\"true\")\n",
    "   .saveAsTable(\"mining.silver_flotation\"))\n",
    "\n",
    "print(\"✅ Silver rebuilt with real timestamps\")\n",
    "display(df.select(\"ts\", \"percent_silica_concentrate\").limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdab58cf-033c-4e86-bbd5-bace8ce3c724",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ===========================================================\n",
    "# GOLD: daily silica KPIs (now with real 'day')\n",
    "# ===========================================================\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "silver = spark.table(\"mining.silver_flotation\")\n",
    "\n",
    "daily_kpis = (silver\n",
    "    .withColumn(\"day\", F.to_date(\"ts\"))\n",
    "    .groupBy(\"day\")\n",
    "    .agg(\n",
    "        F.avg(\"percent_silica_concentrate\").alias(\"avg_silica\"),\n",
    "        F.max(\"percent_silica_concentrate\").alias(\"max_silica\"),\n",
    "        F.min(\"percent_silica_concentrate\").alias(\"min_silica\"),\n",
    "        (F.sum(F.when(F.col(\"percent_silica_concentrate\") > 2.0, 1).otherwise(0))\n",
    "         / F.count(\"*\") * 100).alias(\"pct_above_threshold\"),\n",
    "        F.count(\"*\").alias(\"records\")\n",
    "    )\n",
    "    .orderBy(\"day\"))\n",
    "\n",
    "(daily_kpis.write\n",
    "   .format(\"delta\")\n",
    "   .mode(\"overwrite\")\n",
    "   .option(\"overwriteSchema\",\"true\")\n",
    "   .saveAsTable(\"mining.gold_daily_kpis\"))\n",
    "\n",
    "print(\"✅ Gold rebuilt with daily grouping\")\n",
    "display(daily_kpis.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06370344-7075-4893-8c44-e86f937d4e02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Key insights from this sample:\n",
    "# - On 2017-03-10, average silica was only ~1.74% and\n",
    "#   just 22% of readings were above the 2% threshold.\n",
    "#   This indicates a stable, high-quality production day.\n",
    "#\n",
    "# - On 2017-03-14, average silica jumped to ~3.26% with\n",
    "#   nearly 80% of readings above threshold, signaling a\n",
    "#   potential operational issue that would require attention.\n",
    "#\n",
    "# - Daily record counts (~4,320 per day) confirm that the\n",
    "#   sensors record every 20 seconds, consistent with real\n",
    "#   industrial process historian data.\n",
    "#\n",
    "# Why this matters for the business:\n",
    "# Instead of sifting through 700,000+ raw rows, engineers\n",
    "# now see a clear, day-by-day picture of process stability\n",
    "# and quality risks. This kind of pipeline directly supports\n",
    "# data-driven decision making in mining operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e71ebbcd-beb1-475c-920d-6fe22a8013b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ===========================================================\n",
    "# STEP 11: Visualization - Daily Silica Trends\n",
    "# ===========================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1) Load Gold daily KPIs\n",
    "gold = spark.table(\"mining.gold_daily_kpis\").toPandas()\n",
    "\n",
    "# 2) Plot average silica per day\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(gold[\"day\"], gold[\"avg_silica\"], marker=\"o\", label=\"Avg Silica (%)\")\n",
    "\n",
    "# 3) Add threshold line at 2%\n",
    "plt.axhline(y=2.0, color=\"red\", linestyle=\"--\", label=\"Threshold (2%)\")\n",
    "\n",
    "# 4) Titles & labels for business readability\n",
    "plt.title(\"Daily Silica Concentration Trends\", fontsize=16)\n",
    "plt.xlabel(\"Day\", fontsize=12)\n",
    "plt.ylabel(\"Silica Concentration (%)\", fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "241798d8-258b-4b78-a3df-e6d892e9587c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This visualization highlights the volatility of silica levels\n",
    "# in the flotation process. The red line marks the 2% threshold\n",
    "# where ore quality begins to deteriorate.\n",
    "#\n",
    "# - We see periods of stability (e.g., March, late May) where\n",
    "#   silica averages are consistently below 2%.\n",
    "# - Other periods (April, August) show frequent threshold\n",
    "#   breaches, with daily averages reaching as high as 4.5%.\n",
    "# - About half of the days cross the threshold, indicating a\n",
    "#   significant quality risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8acb3a1-7162-4c4f-9eed-5287f3c72ef6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ===========================================================\n",
    "# Visualization - % of Readings Above Threshold\n",
    "# ===========================================================\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1) Load Gold table as Pandas\n",
    "gold = spark.table(\"mining.gold_daily_kpis\").toPandas()\n",
    "\n",
    "# 2) Plot bar chart\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.bar(gold[\"day\"], gold[\"pct_above_threshold\"], color=\"orange\")\n",
    "\n",
    "# 3) Titles & labels\n",
    "plt.title(\"% of Daily Readings Above 2% Silica\", fontsize=16)\n",
    "plt.xlabel(\"Day\", fontsize=12)\n",
    "plt.ylabel(\"Percent of Readings Above Threshold (%)\", fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.axhline(y=50, color=\"red\", linestyle=\"--\", label=\"50% of day above risk level\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f0c15d9-5b67-4170-bcd1-4550e229286e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This bar chart reveals *how much of each day* was spent\n",
    "# above the 2% silica threshold. While averages provide a\n",
    "# general trend, this KPI exposes the severity of instability:\n",
    "#\n",
    "# - Many days exceed 50%, meaning most of the day’s\n",
    "#   production was out of spec.\n",
    "# - On several days, 100% of readings were above 2%,\n",
    "#   signaling critical process issues.\n",
    "# - Only a few days show low-risk operation (<20%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6db9d135-8264-4cbf-ac17-8bb31072bf69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ===========================================================\n",
    "#  Pipeline Monitoring & Data Quality Log\n",
    "# ===========================================================\n",
    "# What this cell does:\n",
    "# - Captures run metadata (time, duration)\n",
    "# - Records row counts for Bronze/Silver/Gold\n",
    "# - Checks data quality (nulls, ranges, % out-of-spec)\n",
    "# - Appends a single record per run to a Delta log table\n",
    "#   => mining.monitoring_log\n",
    "# ===========================================================\n",
    "\n",
    "import time\n",
    "from pyspark.sql import functions as F, types as T\n",
    "\n",
    "start_ts = F.current_timestamp()\n",
    "\n",
    "# 1) Load layers\n",
    "bronze = spark.table(\"mining.bronze_flotation\")\n",
    "silver = spark.table(\"mining.silver_flotation\")\n",
    "gold   = spark.table(\"mining.gold_daily_kpis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13062547-1c2d-47bb-8a8f-35d5e505040d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Row counts (are we processing the expected volume?) ---\n",
    "bronze_rows = bronze.count()\n",
    "silver_rows = silver.count()\n",
    "gold_rows   = gold.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "602799e5-9ab2-4e38-83d5-be69b4d63f46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Time coverage (are timestamps complete, any gaps?) ---\n",
    "ts_min, ts_max = (silver\n",
    "    .agg(F.min(\"ts\").alias(\"ts_min\"), F.max(\"ts\").alias(\"ts_max\"))\n",
    "    .first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7fd81ac-3e7b-4aa3-9f57-f7c98dcd6d88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Null checks on key business fields ---\n",
    "critical_cols = [\"percent_silica_concentrate\", \"percent_iron_concentrate\", \"ore_pulp_ph\"]\n",
    "null_counts = (silver.select([\n",
    "    F.sum(F.when(F.col(c).isNull(), 1).otherwise(0)).alias(f\"null_{c}\") for c in critical_cols\n",
    "]).first().asDict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ca19b30-bfef-4a79-9cf3-8aed6a675bba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Simple business sanity checks ---\n",
    "# Example: pH should always be between 0 and 14\n",
    "ph_out_of_range = silver.filter((F.col(\"ore_pulp_ph\") < 0) | (F.col(\"ore_pulp_ph\") > 14)).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58fbbf5f-a938-4ed9-9b89-423d019f7959",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example: Flows/levels should not be negative\n",
    "nonneg_cols = [\n",
    "    \"starch_flow\",\"amina_flow\",\"ore_pulp_flow\",\n",
    "    \"flotation_column_01_air_flow\",\"flotation_column_02_air_flow\",\"flotation_column_03_air_flow\",\n",
    "    \"flotation_column_04_air_flow\",\"flotation_column_05_air_flow\",\"flotation_column_06_air_flow\",\n",
    "    \"flotation_column_07_air_flow\",\n",
    "    \"flotation_column_01_level\",\"flotation_column_02_level\",\"flotation_column_03_level\",\n",
    "    \"flotation_column_04_level\",\"flotation_column_05_level\",\"flotation_column_06_level\",\n",
    "    \"flotation_column_07_level\"\n",
    "]\n",
    "neg_violations = sum(silver.filter(F.col(c) < 0).count() for c in nonneg_cols if c in silver.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fccdec9-7927-455d-b395-2dba0d378774",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Optional: today's risk metric (if data has current dates) ---\n",
    "THRESHOLD = 2.0\n",
    "daily_today = (silver\n",
    "    .withColumn(\"day\", F.to_date(\"ts\"))\n",
    "    .filter(F.col(\"day\") == F.to_date(F.current_timestamp()))\n",
    "    .agg((F.sum(F.when(F.col(\"percent_silica_concentrate\") > THRESHOLD, 1).otherwise(0)) / F.count(\"*\") * 100)\n",
    "         .alias(\"pct_above_threshold_today\"))\n",
    "    .first())\n",
    "pct_above_threshold_today = None if daily_today is None else daily_today[\"pct_above_threshold_today\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "348bcf2e-8b5e-4d05-9953-bad0d4dcbe07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Build one log row (note: no run_ts yet) ---\n",
    "schema = T.StructType([\n",
    "    T.StructField(\"bronze_rows\", T.LongType(), False),\n",
    "    T.StructField(\"silver_rows\", T.LongType(), False),\n",
    "    T.StructField(\"gold_rows\",   T.LongType(), False),\n",
    "    T.StructField(\"ts_min\", T.TimestampType(), True),\n",
    "    T.StructField(\"ts_max\", T.TimestampType(), True),\n",
    "    T.StructField(\"null_percent_silica_concentrate\", T.LongType(), True),\n",
    "    T.StructField(\"null_percent_iron_concentrate\",   T.LongType(), True),\n",
    "    T.StructField(\"null_ore_pulp_ph\",                T.LongType(), True),\n",
    "    T.StructField(\"ph_out_of_range\", T.LongType(), True),\n",
    "    T.StructField(\"neg_violations\",  T.LongType(), True),\n",
    "    T.StructField(\"silica_threshold\", T.DoubleType(), True),\n",
    "    T.StructField(\"pct_above_threshold_today\", T.DoubleType(), True),\n",
    "])\n",
    "\n",
    "row = [(\n",
    "    bronze_rows, silver_rows, gold_rows,\n",
    "    ts_min, ts_max,\n",
    "    int(null_counts.get(\"null_percent_silica_concentrate\", 0)),\n",
    "    int(null_counts.get(\"null_percent_iron_concentrate\", 0)),\n",
    "    int(null_counts.get(\"null_ore_pulp_ph\", 0)),\n",
    "    int(ph_out_of_range),\n",
    "    int(neg_violations),\n",
    "    float(THRESHOLD),\n",
    "    None if pct_above_threshold_today is None else float(pct_above_threshold_today),\n",
    ")]\n",
    "\n",
    "log_df = spark.createDataFrame(row, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1a80669-8112-404d-86c9-6f349855d309",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add run_ts dynamically (so it's always filled)\n",
    "log_df = log_df.withColumn(\"run_ts\", F.current_timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cacb802-06e6-4e25-9cf0-e2a68161bcde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Save monitoring log (merge schema for future additions) ---\n",
    "writer = (log_df.write.format(\"delta\").mode(\"append\"))\n",
    "if spark.catalog.tableExists(\"mining.monitoring_log\"):\n",
    "    writer = writer.option(\"mergeSchema\", \"true\")\n",
    "\n",
    "writer.saveAsTable(\"mining.monitoring_log\")\n",
    "\n",
    "print(\"✅ Monitoring snapshot saved\")\n",
    "display(spark.table(\"mining.monitoring_log\").orderBy(F.col(\"run_ts\").desc()).limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8ae4e86-a234-4bd2-a95c-b1b11ed3cb3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# Reset the monitoring table once to remove schema conflicts\n",
    "# -----------------------------------------------------------\n",
    "spark.sql(\"DROP TABLE IF EXISTS mining.monitoring_log\")\n",
    "\n",
    "# Recreate it with the clean schema from our current log_df\n",
    "from pyspark.sql import functions as F, types as T\n",
    "\n",
    "# (Rebuild the log_df exactly as in the last cell up to `log_df = spark.createDataFrame(...)`)\n",
    "# ... then add run_ts and write fresh:\n",
    "\n",
    "log_df = log_df.withColumn(\"run_ts\", F.current_timestamp())\n",
    "\n",
    "(log_df.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")        # create fresh\n",
    "    .saveAsTable(\"mining.monitoring_log\"))\n",
    "\n",
    "print(\"✅ monitoring_log recreated with a clean schema\")\n",
    "display(spark.table(\"mining.monitoring_log\").orderBy(F.col(\"run_ts\").desc()).limit(5))\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_ingest",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
